{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e7c45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m704.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425347 sha256=71f9e1f59b5dc670d34a5e8e39a8a84aa312ea2b30f10da1499b36b09d61ba9b\n",
      "  Stored in directory: /Users/samir/Library/Caches/pip/wheels/38/df/61/8c121f50c3cffd77f8178180dd232d90b3b99d1bd61fb6d6be\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77808a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8febe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_391\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_391-b13)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12ae9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Parallel Processing\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79343189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'signal': {'chest': {'ACC': array([[ 0.95539999, -0.222     , -0.55799997],\n",
      "       [ 0.92579997, -0.2216    , -0.55379999],\n",
      "       [ 0.90820003, -0.21960002, -0.53920001],\n",
      "       ...,\n",
      "       [ 0.87179995, -0.12379998, -0.30419999],\n",
      "       [ 0.87300003, -0.12339997, -0.30260003],\n",
      "       [ 0.87020004, -0.12199998, -0.30220002]]), 'ECG': array([[ 0.02142334],\n",
      "       [ 0.02032471],\n",
      "       [ 0.01652527],\n",
      "       ...,\n",
      "       [-0.00544739],\n",
      "       [ 0.00013733],\n",
      "       [ 0.0040741 ]]), 'EMG': array([[-0.00444031],\n",
      "       [ 0.00434875],\n",
      "       [ 0.00517273],\n",
      "       ...,\n",
      "       [-0.01716614],\n",
      "       [-0.02897644],\n",
      "       [-0.02357483]]), 'EDA': array([[5.25054932],\n",
      "       [5.26733398],\n",
      "       [5.24330139],\n",
      "       ...,\n",
      "       [0.36048889],\n",
      "       [0.36582947],\n",
      "       [0.365448  ]]), 'Temp': array([[30.120758],\n",
      "       [30.129517],\n",
      "       [30.138214],\n",
      "       ...,\n",
      "       [31.459229],\n",
      "       [31.484283],\n",
      "       [31.456268]], dtype=float32), 'Resp': array([[-1.14898682],\n",
      "       [-1.12457275],\n",
      "       [-1.15203857],\n",
      "       ...,\n",
      "       [-1.10321045],\n",
      "       [-1.08642578],\n",
      "       [-1.09710693]])}, 'wrist': {'ACC': array([[ 62., -21., 107.],\n",
      "       [ 66.,  13.,  53.],\n",
      "       [ 41.,   9.,  15.],\n",
      "       ...,\n",
      "       [ 41.,  25.,  11.],\n",
      "       [ 39.,  27.,  22.],\n",
      "       [ 56.,  26.,  10.]]), 'BVP': array([[-59.37],\n",
      "       [-53.42],\n",
      "       [-44.4 ],\n",
      "       ...,\n",
      "       [ 18.26],\n",
      "       [ 18.68],\n",
      "       [ 19.71]]), 'EDA': array([[1.138257],\n",
      "       [1.125444],\n",
      "       [1.011405],\n",
      "       ...,\n",
      "       [0.059208],\n",
      "       [0.073303],\n",
      "       [0.045113]]), 'TEMP': array([[35.41],\n",
      "       [35.41],\n",
      "       [35.41],\n",
      "       ...,\n",
      "       [34.23],\n",
      "       [34.23],\n",
      "       [34.23]])}}, 'label': array([0, 0, 0, ..., 0, 0, 0], dtype=int32), 'subject': 'S2'}\n"
     ]
    }
   ],
   "source": [
    "root_data_path = \"/Users/samir/Downloads/WESAD\"\n",
    "\n",
    "# Specify the subject ID you want to work with\n",
    "subject_id = \"S2\"\n",
    "\n",
    "# Define the paths to relevant files for the subject\n",
    "readme_path = f\"{root_data_path}/{subject_id}/{subject_id}_readme.txt\"\n",
    "csv_path = f\"{root_data_path}/{subject_id}/{subject_id}_quest.csv\"\n",
    "text_path = f\"{root_data_path}/{subject_id}/{subject_id}_respiban.txt\"\n",
    "zip_path = f\"{root_data_path}/{subject_id}/{subject_id}_E4_Data.zip\"\n",
    "pickle_path = f\"{root_data_path}/{subject_id}/{subject_id}.pkl\"\n",
    "\n",
    "# Read and process the data for the subject\n",
    "# You can customize this part based on your dataset structure and analysis requirements\n",
    "\n",
    "# Read README file\n",
    "readme_df = spark.read.text(readme_path)\n",
    "\n",
    "# Read CSV file\n",
    "csv_df = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "\n",
    "# Read text file\n",
    "text_df = spark.read.text(text_path)\n",
    "\n",
    "# Unzip and read contents from ZIP archive\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"/Users/samir/Downloads/WESAD/S2\")\n",
    "\n",
    "e4_data_df = spark.read.text(\"/Users/samir/Downloads/WESAD/S2/S2_readme.txt\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    pickle_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Provide the path to your Pickle file\n",
    "pickle_file_path = \"/Users/samir/Downloads/WESAD/S2/S2.pkl\"\n",
    "\n",
    "\n",
    "# Print the data\n",
    "print(pickle_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edeae45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
